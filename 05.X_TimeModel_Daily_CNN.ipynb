{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Financial Model (Daily)\n",
    "Daniel Ruiz, MSc in Data Science and Business Analytics (DSBA), Bocconi University\n",
    "\n",
    "Reference codes (alphabetically):\n",
    "- https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python\n",
    "- https://machinelearningmastery.com/how-to-develop-convolutional-neural-network-models-for-time-series-forecasting/\n",
    "- https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/\n",
    "- https://pythonprogramming.net/convolutional-neural-network-deep-learning-python-tensorflow-keras/\n",
    "- https://stackoverflow.com/questions/32419510/how-to-get-reproducible-results-in-keras\n",
    "- https://towardsdatascience.com/how-to-use-convolutional-neural-networks-for-time-series-classification-56b1b0a07a57\n",
    "\n",
    "\n",
    "## 3.1. Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from numpy import array, hstack, vstack\n",
    "\n",
    "# graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# neural networks\n",
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(sequences, n_steps):\n",
    "    # split a multivariate sequence into samples\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "def split_data(df,perc_train,perc_valid):\n",
    "    assert perc_train + perc_valid <= 1\n",
    "    assert perc_train >= 0\n",
    "    assert perc_valid >= 0\n",
    "    train = int(len(X[0])*perc_train)\n",
    "    valid = int(len(X[0])*(perc_train+perc_valid))\n",
    "    df_train = X[:train]\n",
    "    df_valid = X[train:valid]\n",
    "    df_test = X[valid:]\n",
    "    return df_train, df_valid, df_test\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "def return_confusion(company,y_test, predicted_classes):\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(y_test,predicted_classes)\n",
    "    TN, FP, FN, TP = cm.flatten()\n",
    "    total = TN+FP+FN+TP\n",
    "\n",
    "    # class 1\n",
    "    prec1 = TP / (TP+FP)\n",
    "    reca1 = TP / (TP+FN)\n",
    "    fone1 = 2*(prec1*reca1)/(prec1+reca1)\n",
    "    # class 0\n",
    "    prec0 = TN / (TN+FN)\n",
    "    reca0 = TN / (TN+FP)\n",
    "    fone0 = 2*(prec0*reca0)/(prec0+reca0)\n",
    "\n",
    "    # global / weighted\n",
    "    accuw = TP/total +TN/total\n",
    "    precw = prec0*(TN+FP)/(total) + prec1*(TP+FN)/(total)\n",
    "    recaw = reca0*(TN+FP)/(total) + reca1*(TP+FN)/(total)\n",
    "    fonew = fone0*(TN+FP)/(total) + fone1*(TP+FN)/(total)\n",
    "\n",
    "    # list\n",
    "    sup = [company, TN, FP, FN, TP, prec1, reca1, fone1, prec0, reca0, fone0, precw, recaw, fonew, accuw]\n",
    "\n",
    "    return sup\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "def save_graphs(model_train,company,folder='estimation_daily/'):\n",
    "    \n",
    "    # performance in validation and test set\n",
    "    metricas = [i for i in list(model_train.history.keys()) if i[:4]!='val_']\n",
    "    for metrica in metricas:\n",
    "        met = model_train.history[metrica]\n",
    "        val_met = model_train.history['val_'+metrica]\n",
    "        epochs = range(len(met))\n",
    "        plt.figure(figsize=(15,5))\n",
    "        sns.set(font_scale=1.5)\n",
    "        plt.plot(epochs, met, 'b', c='orange', label='Training '+metrica,lw=1)\n",
    "        plt.plot(epochs, val_met, 'b', c='green', label='Validation '+metrica,lw=1)\n",
    "        plt.title('Training and validation '+metrica)\n",
    "        plt.legend(loc='center left')\n",
    "        plt.savefig(folder+company+'_'+metrica+'.png')\n",
    "        plt.close()\n",
    "        \n",
    "#------------------------------------------------------------------------\n",
    "def find_cut(model,X,y):\n",
    "                \n",
    "    predictions = model.predict(X)[:,1]\n",
    "\n",
    "    total = len(predictions)\n",
    "    max_acc=0\n",
    "    min_acc=1\n",
    "\n",
    "    # loop\n",
    "    sup_min = max(int(min(predictions)*100)+1,33)\n",
    "    sup_max = min(int(max(predictions)*100)-1,67)\n",
    "    best_cut = sup_min\n",
    "\n",
    "    for cut in range(sup_min,sup_max): \n",
    "\n",
    "        hits = sum((predictions>=(cut/100))==y)\n",
    "\n",
    "        acc = hits/total\n",
    "\n",
    "        if acc >= max_acc:\n",
    "            best_cut = cut\n",
    "            max_acc = acc\n",
    "\n",
    "        if acc <= min_acc:\n",
    "            worst_cut = cut\n",
    "            min_acc = acc\n",
    "            \n",
    "    return max_acc, best_cut, min_acc, worst_cut\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "def prepare_model_bin(n_variables,n_steps=3,n_features=1,num_classes=2):\n",
    "    \n",
    "    def make_cnn(n_steps=n_steps,n_features=n_features):\n",
    "        visible = Input(shape=(n_steps, n_features))\n",
    "        cnn = Conv1D(filters=16, kernel_size=2, activation='relu')(visible)\n",
    "        cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "        cnn = Dropout(0.25)(cnn)\n",
    "        cnn = Flatten()(cnn)\n",
    "        return visible, cnn\n",
    "\n",
    "    # generate input models for each variable\n",
    "    visibles, cnns = [], []\n",
    "    for i in range(n_variables):\n",
    "    # add element\n",
    "        visibles.append([])\n",
    "        cnns.append([])\n",
    "        visibles[-1], cnns[-1] = make_cnn()\n",
    "\n",
    "    # merge input models\n",
    "    merge = concatenate(cnns)\n",
    "    dense = Dense(50, activation='relu')(merge)\n",
    "    dense = Dropout(0.3)(dense)\n",
    "    output = Dense(num_classes, activation='softmax')(dense)\n",
    "    model = Model(inputs=visibles, outputs=output)\n",
    "\n",
    "    # for continuous predictions\n",
    "    metricas = ['accuracy','mse','msle','mae', 'mape', 'cosine']\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=metricas)\n",
    "        \n",
    "    return(model)\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "def prepare_model_cont(n_variables,n_steps=3,n_features=1):\n",
    "    \n",
    "    def make_cnn(n_steps=n_steps,n_features=n_features):\n",
    "        visible = Input(shape=(n_steps, n_features))\n",
    "        cnn = Conv1D(filters=16, kernel_size=2, activation='relu')(visible)\n",
    "        cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "        cnn = Dropout(0.25)(cnn)\n",
    "        cnn = Flatten()(cnn)\n",
    "        return visible, cnn\n",
    "\n",
    "    # generate input models for each variable\n",
    "    visibles, cnns = [], []\n",
    "    for i in range(n_variables):\n",
    "    # add element\n",
    "        visibles.append([])\n",
    "        cnns.append([])\n",
    "        visibles[-1], cnns[-1] = make_cnn()\n",
    "\n",
    "    # merge input models\n",
    "    merge = concatenate(cnns)\n",
    "    dense = Dense(50, activation='relu')(merge)\n",
    "    dense = Dropout(0.3)(dense)\n",
    "    output = Dense(1)(dense)\n",
    "    model = Model(inputs=visibles, outputs=output)\n",
    "\n",
    "    # for continuous predictions\n",
    "    metricas = ['accuracy','mse','msle','mae', 'mape', 'cosine']\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=metricas)\n",
    "        \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2  Loading and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds\n",
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# companies\n",
    "my_companies = ['br_embraer',                \n",
    "                'br_americanas',\n",
    "                'br_pontofrio',\n",
    "                'br_petrobras',\n",
    "                'br_bradesco',\n",
    "                'br_renner',\n",
    "                'br_gol',\n",
    "                'br_magazineluiza',\n",
    "                'br_itau',\n",
    "                'us_abercrombie',\n",
    "                'us_boeing',\n",
    "                'us_beyondmeat',\n",
    "                'us_morganstanley',\n",
    "                'us_jpmorgan',\n",
    "                'us_exxonmobil',\n",
    "                'us_americanair',\n",
    "                'us_cocacola',\n",
    "                'us_tesla',\n",
    "                'us_wsj']\n",
    "\n",
    "my_companies=['us_cocacola']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    }
   ],
   "source": [
    "folder = 'Models_Daily/CNN_Cont/'\n",
    "name = 'performance_'+time.strftime('%Y-%m-%d_%H-%M',time.gmtime())\n",
    "\n",
    "with open(folder+name+'.csv', 'w', encoding=\"utf-8\") as csvFile:\n",
    "\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    \n",
    "    for company in my_companies:\n",
    "        \n",
    "        print(company)\n",
    "    \n",
    "        # open data\n",
    "        df = pd.read_pickle('Dataset_ToModel_daily/'+company+'.pkl')\n",
    "\n",
    "        # select variables\n",
    "        X = df[['final_pos_off','final_neg_off','avg_pos_off']]\n",
    "        y = df['l_close_to_close']\n",
    "\n",
    "        # data format\n",
    "        in_seqs = [array(X[col]).reshape((len(y), 1)) for col in X.columns]\n",
    "        out_seq = [array(y).reshape((len(y), 1))]\n",
    "        dataset = hstack(tuple(in_seqs+out_seq))\n",
    "        n_steps=3\n",
    "        X, y = split_sequences(dataset, n_steps)\n",
    "        X = [X[:, :, i].reshape(X.shape[0], X.shape[1], 1) for i in range(X.shape[2])]\n",
    "\n",
    "        # train, valid, test\n",
    "        perc_train=0.50\n",
    "        perc_valid=0.25\n",
    "        train=int(perc_train*len(X[0]))\n",
    "        valid=int((perc_train+perc_valid)*len(X[0]))\n",
    "        # X\n",
    "        X_train = [X[i][:train] for i in range(len(X))]\n",
    "        X_valid = [X[i][train:valid] for i in range(len(X))]\n",
    "        X_test = [X[i][valid:] for i in range(len(X))]\n",
    "        # y\n",
    "        y_train = y[:train]\n",
    "        y_valid = y[train:valid]\n",
    "        y_test = y[valid:]\n",
    "\n",
    "        # model\n",
    "        model_cont = prepare_model_cont(3)\n",
    "        model_cont_train = model_cont.fit(X_train,\n",
    "                                          y_train,\n",
    "                                          epochs=100,\n",
    "                                          verbose=0,\n",
    "                                          validation_data=(X_valid, y_valid))\n",
    "        sup = return_confusion(company,y_test>0,model_cont.predict(X_test)>0)\n",
    "\n",
    "        # save data\n",
    "        csvWriter.writerow(sup)\n",
    "        save_graphs(model_cont_train,company,folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# binary\n",
    "model_bin = prepare_model_bin(3)\n",
    "#model_bin.fit(X,,epochs=20,verbose=0) #validation_data=(X_valid, to_categorical(y_valid)))\n",
    "\n",
    "\n",
    "model_bin_train = model_bin.fit(X_train,\n",
    "                                to_categorical(y_train>=0),\n",
    "                                epochs=100,\n",
    "                                verbose=0,\n",
    "                                validation_data=(X_valid, to_categorical(y_valid>=0)>0))\n",
    "\n",
    "sup = return_confusion(company,y_test>0,model_bin.predict(X_test)[:,1]>=0.50)\n",
    "#csvWriter.writerow(sup)\n",
    "\n",
    "#save_graphs(model_bin_train,company,'model_daily_bin/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
