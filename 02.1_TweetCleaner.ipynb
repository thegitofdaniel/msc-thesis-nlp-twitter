{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleaning tweets\n",
    "\n",
    "Daniel Ruiz, MSc in Data Science and Business Analytics (DSBA), Bocconi University\n",
    "\n",
    "Reference codes (alphabetically):\n",
    "- https://www.kaggle.com/eliasdabbas/how-to-create-a-regex-to-extract-emoji\n",
    "- https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize\n",
    "- https://www.nltk.org/howto/portuguese_en.html\n",
    "- https://py-googletrans.readthedocs.io/en/latest/#googletrans.models.Detected\n",
    "- https://stackoverflow.com/questions/49498801/python-googletrans-library-no-json-object-could-be-decoded\n",
    "- https://towardsdatascience.com/creating-the-twitter-sentiment-analysis-program-in-python-with-naive-bayes-classification-672e5589a7ed\n",
    "\n",
    "## 2.1. Loading packages and hard data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "# language identification\n",
    "from langdetect import detect\n",
    "\n",
    "# stemmer\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Defining funtions\n",
    "\n",
    "### Tweet Basic Information\n",
    "- Number of characters: Counting tweet characters is not as immediate as one could think. Tweepy returns a string that has many special characters, including the textual representation of line breaks, emojis, etc. Counting these characters would create an upward bias to the number of characters. To count the 'true' number of characters, it is neccessary to convert everything to 'traditional' characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hashtags(tweet,not_this=''):\n",
    "    # input = tweet = string\n",
    "    # output = list of strings\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', r' ', tweet)\n",
    "    return [i  for i in tweet.split() if i.startswith(\"#\")]\n",
    "\n",
    "def count_characters(tweet):\n",
    "    \n",
    "    # our number is biased downwards\n",
    "    \n",
    "    # images and URLs have the same structure (a link)\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', r' ', tweet)\n",
    "    \n",
    "    # RT\n",
    "    tweet = re.sub('^rt @[^\\s]+', r' ', tweet)\n",
    "    \n",
    "    # usernames -> disappear\n",
    "    tweet = re.sub('@[^\\s]+', r' ', tweet)\n",
    "\n",
    "    # Emoji patterns\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    tweet = re.sub(emoji_pattern,r' ',tweet)\n",
    "    \n",
    "    # Retain only alpha-numeric characters, underline and dash (e.g. punctuation)\n",
    "    tweet = re.sub(r'[^A-Za-z0-9]+', ' ', tweet)\n",
    "    \n",
    "    return len(tweet)\n",
    "\n",
    "def count_tokens(tweet):\n",
    "    return len(tweet.split())\n",
    "\n",
    "def is_retweet(tweet):\n",
    "    if re.match('^rt @[^\\s]+',tweet)!=None:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def extract_bigrams(words):\n",
    "    \n",
    "    # input = list of strings\n",
    "    \n",
    "    extracted = []\n",
    "    \n",
    "    if len(words)<2:\n",
    "        return extracted\n",
    "    else:\n",
    "        for i in range(1,len(words)):\n",
    "            extracted.append(words[i-1]+'_'+words[i])\n",
    "\n",
    "    return extracted\n",
    "\n",
    "def remove_pt_accents(tweet):\n",
    "\n",
    "    tweet = re.sub('(À)|(à)|(Á)|(á)|(Â)|(â)|(Ã)|(ã)', 'a', tweet)\n",
    "    tweet = re.sub('(É)|(é)|(Ê)|(ê)', 'e', tweet)\n",
    "    tweet = re.sub('(Í)|(í)', 'i', tweet)\n",
    "    tweet = re.sub('(Ó)|(ó)|(Ô)|(ô)|(Õ)|(õ)', 'o', tweet)\n",
    "    tweet = re.sub('(Ú)|(ú)|(Ü)|(ü)', 'u', tweet)\n",
    "    tweet = re.sub('(Ç)|(ç)', 'c', tweet)\n",
    "    tweet = re.sub('(Ñ)|(ñ)', 'n', tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Processing the raw database\n",
    "\n",
    "1. PreCleaner\n",
    "        First, we open the raw files and remove duplicates that might have been generated during the scrapping process. We know whether a tweet has been scrapped two times because each tweet has an unique ID.\n",
    "\n",
    "2. TweetCleaner\n",
    "        Second, we clean the data and apply the functions created in the previous section.\n",
    "\n",
    "3. TweetStemmer\n",
    "        Third, we reduce words to stems using the snowball stemmer. NLTK contains lemmatization in English, but not in Portuguese. Thus, we've preferred to run only the stemming (and not lemmatization), as it is available on both languages (and the other is not).\n",
    "\n",
    "4. TweetLanguage\n",
    "        Fourth, we detect the language of the tweet using a preliminary output of the Cleaner. THis function comes last because it takes more time.\n",
    "\n",
    "5. TweetFilter\n",
    "        Fifth, we filter tweets that are presumably non-informative. The filters parameters are hard-codes extracted from TweetCollectiveAnalysis. Particularly, we filter: (i) users that look fake (less than 50 followers) and (ii) tweets with 0 stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TweetPreCleaner(company):\n",
    "    \n",
    "    # names columns\n",
    "    # remove duplicates\n",
    "    # all to lower-case\n",
    "    # remove usernames, urls\n",
    "    # saves in Clean_01\n",
    "    \n",
    "    # timing (start)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # open and read file\n",
    "    filename='Dataset_Twitter/'+company[0]+'.csv'\n",
    "    df = pd.read_csv(filename,header=None)\n",
    "    print(company[0])\n",
    "    \n",
    "    if len(df.columns)==5:\n",
    "        df.columns = ['text','id','datetime','user_name','user_followers']\n",
    "    else:\n",
    "        df.columns = ['text','id','datetime','user_name','user_followers','sentiment_pos','query_used']\n",
    "\n",
    "    # remove duplicates\n",
    "    print(df.shape)\n",
    "    df = df.drop_duplicates('id')\n",
    "    print(df.shape)\n",
    "\n",
    "    # this will be useful later on\n",
    "    df['text']=df.apply(lambda x: x.text.lower(), axis=1)\n",
    "    df['user_name']=df.apply(lambda x: x.user_name.lower(), axis=1)\n",
    "    \n",
    "    # save in new folder\n",
    "    df.to_pickle('Dataset_Twitter_Clean_01/'+company[0]+'.pkl')\n",
    "    \n",
    "    # timing (end)\n",
    "    t_sec = round(time.time() - start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60) \n",
    "    print('Time passed: {}hour:{}min:{}sec'.format(t_hour,t_min,t_sec))\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"boeing is building a brand new 747 air force one for future presidents, but costs are out of control, more than $4 billion. cancel order!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boeing', 'building', 'brand', 'new', 'air', 'force', 'one', 'future', 'presidents', 'costs', 'control', 'billion', 'cancel', 'order']\n"
     ]
    }
   ],
   "source": [
    "def TweetCleaner(company):\n",
    "    \n",
    "    # emoticons\n",
    "    # Happy emoticons -> , removed because of confusion\n",
    "    emoticons_happy = set([\n",
    "        ':-)', ':)', ';)', ':o)', ':]',\n",
    "        ':3', ':c)', ':>', ':}','8)',\n",
    "        '=)', '=]', ':^)', ':-d', ':d',\n",
    "        '8-d', '8d',':b','d:',\n",
    "        '=-d', '=d','=-3', '=3',\n",
    "        ':-))', \":'-)\", \":')\", ':*', ':^*',\n",
    "        '>:p', ':-p', ':p', ':-p', ':p',\n",
    "        '=p', ':-b', ':b', '>:)', '>;)',\n",
    "        '>:-)','<3','xp','xd','x-p','x-d'\n",
    "        ])\n",
    "\n",
    "    # Sad emoticons\n",
    "    emoticons_sad = set([\n",
    "        ':L', ':-/', '>:/', ':S', '>:[',\n",
    "        ':@', ':-(', ':[', ':-||', '=l',\n",
    "        ':<', ':-[', ':-<', '=\\\\', '=/',\n",
    "        '>:(', ':(', '>.<', \":'-(\", \":'(\",\n",
    "        ':\\\\', ':-c', ':c', ':{', '>:\\\\',\n",
    "        ';('\n",
    "        ])\n",
    "\n",
    "    # Emoji patterns\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    tweets = [company]\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        # urls, usernames -> disappear\n",
    "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', ' ', tweet)\n",
    "        tweet = re.sub('@[^\\s]+', ' ', tweet)\n",
    "        \n",
    "        # symbols  -> ' '\n",
    "        tweet = re.sub(emoji_pattern, ' ',tweet) # emojis\n",
    "        tweet = re.sub('#([^\\s]+)', '\\\\1', tweet) # hash symbol\n",
    "        tweet = re.sub('via twitter', ' ', tweet) # 'via Twitter'\n",
    "        tweet = re.sub('\\\\brt\\\\b', ' ', tweet) # RT\n",
    "        tweet = re.sub('\\\\b[0-9]+\\\\b\\\\s*', ' ', tweet) # number-only words\n",
    "        tweet = re.sub('‚Ä¶', ' ', tweet) # mentions\n",
    "    \n",
    "        # portuguese characters -> remove special signs\n",
    "        tweet = remove_pt_accents(tweet)\n",
    "\n",
    "        # Retain only alpha-numeric characters (e.g. punctuation)\n",
    "        tweet = re.sub(r\"'s \", ' ', tweet)\n",
    "        tweet = re.sub(r\"'re \", ' ', tweet)\n",
    "        tweet = re.sub(r\"'ve \", ' ', tweet)\n",
    "        tweet = re.sub(r\"'\", '', tweet)\n",
    "        tweet = re.sub(r'[^A-Za-z0-9]+', ' ', tweet)\n",
    "\n",
    "        # tokenize (i.e. repeated characters)\n",
    "        tweet = word_tokenize(tweet)\n",
    "\n",
    "        # stopwords\n",
    "        comp_lang = 'us'\n",
    "        if comp_lang == 'br':\n",
    "            stop_xx = stopwords.words('portuguese') + ['pra','voc','vc']\n",
    "            stop_xx.remove('não')\n",
    "            stop_xx.remove('nem')\n",
    "            stop_xx = set([remove_pt_accents(word) for word in stop_xx])\n",
    "            negations=set(['nao','nem','n'])\n",
    "\n",
    "        else:\n",
    "            stop_xx = stopwords.words('english')[:143] + []\n",
    "            stop_xx.remove(\"don\")\n",
    "            stop_xx.remove(\"don't\")\n",
    "            stop_xx.remove(\"nor\")\n",
    "            stop_xx.remove(\"no\")\n",
    "            stop_xx.remove(\"not\")\n",
    "            stop_xx = [re.sub(r\"'\", '', word) for word in stop_xx]\n",
    "            stop_XX = set(stop_xx)\n",
    "            negations = set(stopwords.words('english')[143:]+[\"don\",\"dont\",\"not\",\"nor\",\"no\"])\n",
    "            \n",
    "        tweet = [word for word in tweet if (len(word)>1 or word=='n')]\n",
    "        tweet = [word for word in tweet if (word not in stop_xx)]\n",
    "        \n",
    "        print(tweet)\n",
    "    \n",
    "TweetCleaner(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TweetCleaner(company):\n",
    "    \n",
    "    # extracts info and pre-process text\n",
    "    \n",
    "    # timing (start)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # open and read file\n",
    "    filename='Dataset_Twitter_Clean_01/'+company[0]+'.pkl'\n",
    "    df = pd.read_pickle(filename)\n",
    "    print(company[0])\n",
    "\n",
    "    # local variables\n",
    "    comp_lang = company[0][:2]\n",
    "    comp_name = company[1].lower()[1:]\n",
    "    comp_hash = company[2].lower()\n",
    "\n",
    "    # extract info\n",
    "    df['hashtags']=df.apply(lambda x: extract_hashtags(x.text, comp_hash), axis=1)\n",
    "    df['is_retweet']=df.apply(lambda x: is_retweet(x.text), axis=1)\n",
    "    \n",
    "    # emoticons\n",
    "    # Happy emoticons -> , removed because of confusion\n",
    "    emoticons_happy = set([\n",
    "        ':-)', ':)', ';)', ':o)', ':]',\n",
    "        ':3', ':c)', ':>', ':}','8)',\n",
    "        '=)', '=]', ':^)', ':-d', ':d',\n",
    "        '8-d', '8d',':b','d:',\n",
    "        '=-d', '=d','=-3', '=3',\n",
    "        ':-))', \":'-)\", \":')\", ':*', ':^*',\n",
    "        '>:p', ':-p', ':p', ':-p', ':p',\n",
    "        '=p', ':-b', ':b', '>:)', '>;)',\n",
    "        '>:-)','<3','xp','xd','x-p','x-d'\n",
    "        ])\n",
    "\n",
    "    # Sad emoticons\n",
    "    emoticons_sad = set([\n",
    "        ':L', ':-/', '>:/', ':S', '>:[',\n",
    "        ':@', ':-(', ':[', ':-||', '=l',\n",
    "        ':<', ':-[', ':-<', '=\\\\', '=/',\n",
    "        '>:(', ':(', '>.<', \":'-(\", \":'(\",\n",
    "        ':\\\\', ':-c', ':c', ':{', '>:\\\\',\n",
    "        ';('\n",
    "        ])\n",
    "\n",
    "    # Emoji patterns\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    # new variables\n",
    "    has_happy = []\n",
    "    has_sad = []\n",
    "    text_lang_detect=[]\n",
    "    text_clean = []\n",
    "    \n",
    "    # loop (data struture = list for speed)\n",
    "    user_i=0\n",
    "    user_name = df['user_name'].tolist()\n",
    "    tweets = df['text'].tolist()\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        # urls, usernames -> disappear\n",
    "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', ' ', tweet)\n",
    "        tweet = re.sub('@[^\\s]+', ' ', tweet)\n",
    "        \n",
    "        # emoticons -> remove (but record in variable)\n",
    "\n",
    "        tweet_sup = []\n",
    "        happy = 0\n",
    "        sad = 0\n",
    "        \n",
    "        tweet = tweet.split()\n",
    "        for token in tweet:\n",
    "            if token in emoticons_happy:\n",
    "                tweet_sup.append('')\n",
    "                happy+=1\n",
    "            elif token in emoticons_sad:\n",
    "                tweet_sup.append('')\n",
    "                sad+=1\n",
    "            else:\n",
    "                tweet_sup.append(token)\n",
    "        \n",
    "        tweet = \" \".join(tweet_sup)\n",
    "        has_happy.append(happy)\n",
    "        has_sad.append(sad)\n",
    "        \n",
    "        # symbols  -> ' '\n",
    "        tweet = re.sub(emoji_pattern, ' ',tweet) # emojis\n",
    "        tweet = re.sub('#([^\\s]+)', '\\\\1', tweet) # hash symbol\n",
    "        tweet = re.sub('via twitter', ' ', tweet) # 'via Twitter'\n",
    "        tweet = re.sub('\\\\brt\\\\b', ' ', tweet) # RT\n",
    "        tweet = re.sub('\\\\b[0-9]+\\\\b\\\\s*', ' ', tweet) # number-only words\n",
    "        tweet = re.sub('‚Ä¶', ' ', tweet) # mentions\n",
    "        tweet = re.sub(user_name[user_i], ' ', tweet) # poster's username\n",
    "        tweet = re.sub(comp_name, ' ', tweet) # company's name\n",
    "        \n",
    "        # create intermediary output for language detection\n",
    "        text_lang_detect.append(tweet)\n",
    "    \n",
    "        # portuguese characters -> remove special signs\n",
    "        tweet = remove_pt_accents(tweet)\n",
    "\n",
    "        # Retain only alpha-numeric characters (e.g. punctuation)\n",
    "        tweet = re.sub(r\"'s \", ' ', tweet)\n",
    "        tweet = re.sub(r\"'re \", ' ', tweet)\n",
    "        tweet = re.sub(r\"'ve \", ' ', tweet)\n",
    "        tweet = re.sub(r\"'\", '', tweet)\n",
    "        tweet = re.sub(r'[^A-Za-z0-9]+', ' ', tweet)\n",
    "\n",
    "        # tokenize (i.e. repeated characters)\n",
    "        tweet = word_tokenize(tweet)\n",
    "\n",
    "        # stopwords\n",
    "        if comp_lang == 'br':\n",
    "            stop_xx = stopwords.words('portuguese') + ['pra','voc','vc']\n",
    "            stop_xx.remove('não')\n",
    "            stop_xx.remove('nem')\n",
    "            stop_xx = set([remove_pt_accents(word) for word in stop_xx])\n",
    "            negations=set(['nao','nem','n'])\n",
    "\n",
    "        else:\n",
    "            stop_xx = stopwords.words('english')[:143] + []\n",
    "            stop_xx.remove(\"don\")\n",
    "            stop_xx.remove(\"don't\")\n",
    "            stop_xx.remove(\"nor\")\n",
    "            stop_xx.remove(\"no\")\n",
    "            stop_xx.remove(\"not\")\n",
    "            stop_xx = [re.sub(r\"'\", '', word) for word in stop_xx]\n",
    "            stop_XX = set(stop_xx)\n",
    "            negations = set(stopwords.words('english')[143:]+[\"don\",\"dont\",\"not\",\"nor\",\"no\"])\n",
    "            \n",
    "        tweet = [word for word in tweet if (len(word)>1 or word=='n')]\n",
    "        tweet = [word for word in tweet if (word not in stop_xx)]\n",
    "        \n",
    "        # negations handling\n",
    "        if (len(tweet)>=1) and (tweet[-1] in negations):\n",
    "            tweet[-1]=='neg_'\n",
    "            \n",
    "        if len(tweet)>=2:\n",
    "            for i in range(len(tweet)-2,-1,-1):\n",
    "                if tweet[i] in negations:\n",
    "                    tweet.pop(i)\n",
    "                    tweet[i] ='neg_'+tweet[i]\n",
    "                    if tweet[i][:8] == 'neg_neg_':\n",
    "                        tweet[i] = tweet[i][8:]\n",
    "                        \n",
    "        tweet = [word for word in tweet if word!='']\n",
    "\n",
    "\n",
    "        # clean list\n",
    "        text_clean.append(tweet)\n",
    "        \n",
    "        user_i+=1\n",
    "    \n",
    "    # add to dataframe\n",
    "    df['has_happy']=has_happy\n",
    "    df['has_sad']=has_sad\n",
    "    df['text_lang_detect']=text_lang_detect\n",
    "    df['text_clean']=text_clean\n",
    "    \n",
    "    # save in new folder\n",
    "    df.to_pickle('Dataset_Twitter_Clean_01/'+company[0]+'.pkl')\n",
    "    \n",
    "    # timing (end)\n",
    "    t_sec = round(time.time() - start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60) \n",
    "    print('Time passed: {}hour:{}min:{}sec'.format(t_hour,t_min,t_sec))\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TweetStemmer(company):\n",
    "    \n",
    "    # generate stems, create bigtams\n",
    "    \n",
    "    # timing (start)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # open and read file\n",
    "    filename='Dataset_Twitter_Clean_01/'+company[0]+'.pkl'\n",
    "    df = pd.read_pickle(filename)\n",
    "    print(company[0])\n",
    "\n",
    "    # stemmer language\n",
    "    \n",
    "    if company[0][:2] == 'br':\n",
    "        stemmer=SnowballStemmer('portuguese')\n",
    "    else:\n",
    "        stemmer=SnowballStemmer('english')\n",
    "    \n",
    "    tweets = df.text_clean.tolist()\n",
    "    for i in range(len(tweets)):\n",
    "        tweets[i]=[stemmer.stem(word) for word in tweets[i]]\n",
    "    df['snowball_stems']=tweets\n",
    "    \n",
    "    # extract information\n",
    "    \n",
    "    # text\n",
    "    df['count_char']=df.apply(lambda x: count_characters(x.text), axis=1)\n",
    "    df['count_tokens']=df.apply(lambda x: count_characters(x.text), axis=1)\n",
    "    # text_clean\n",
    "    df['count_words']=df.apply(lambda x: len(x.text_clean), axis=1)\n",
    "    df['count_stems']=df.apply(lambda x: len(x.snowball_stems), axis=1)\n",
    "    \n",
    "    # add bigrams\n",
    "    bag1 = []\n",
    "    bag2 = []\n",
    "    \n",
    "    tweets = df.text_clean.tolist()\n",
    "    stems = df.snowball_stems.tolist()\n",
    "    \n",
    "    for i in range(len(tweets)):\n",
    "        bag1.append(list(extract_bigrams(tweets[i])))\n",
    "        bag2.append(list(extract_bigrams(stems[i])))\n",
    "    \n",
    "    df['text_clean_bigrams']=bag1\n",
    "    df['stems_bigrams']=bag2\n",
    "    df['words_unigrams_bigrams']=df['text_clean']+df['text_clean_bigrams']\n",
    "    df['stems_unigrams_bigrams']=df['snowball_stems']+df['stems_bigrams']\n",
    "    \n",
    "    # save in new folder\n",
    "    df.to_pickle('Dataset_Twitter_Clean_01/'+company[0]+'.pkl')\n",
    "    \n",
    "    # timing (end)\n",
    "    t_sec = round(time.time() - start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60) \n",
    "    print('Time passed: {}hour:{}min:{}sec'.format(t_hour,t_min,t_sec))\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ['building', 'brand', 'new', 'air', 'force', 'one', 'future', 'presidents', 'costs', 'control', 'billion', 'cancel', 'order']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['build',\n",
       " 'brand',\n",
       " 'new',\n",
       " 'air',\n",
       " 'forc',\n",
       " 'one',\n",
       " 'futur',\n",
       " 'presid',\n",
       " 'cost',\n",
       " 'control',\n",
       " 'billion',\n",
       " 'cancel',\n",
       " 'order']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[stemmer.stem(word) for word in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TweetLanguage(company):\n",
    "    \n",
    "    # detects the language\n",
    "    \n",
    "    # timing (start)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # open and read file\n",
    "    filename='Dataset_Twitter_Clean_01/'+company[0]+'.pkl'\n",
    "    df = pd.read_pickle(filename)\n",
    "    print(company[0])\n",
    "\n",
    "    # language\n",
    "    tweet_lang = []\n",
    "    tweet_en = []\n",
    "    tweet_pt = []\n",
    "    \n",
    "    tweets = df.text_lang_detect.tolist()\n",
    "\n",
    "    for tweet in tweets:\n",
    "    \n",
    "        try:\n",
    "            lang = detect(tweet)\n",
    "            if lang=='pt':\n",
    "                pt=1\n",
    "                en=0\n",
    "            elif lang=='en':\n",
    "                pt=0\n",
    "                en=1\n",
    "            else:\n",
    "                pt=0\n",
    "                en=0\n",
    "        except:\n",
    "            lang = 'xx'\n",
    "            pt = 0\n",
    "            en = 0\n",
    "            \n",
    "        tweet_pt.append(pt)\n",
    "        tweet_en.append(en)\n",
    "        tweet_lang.append(lang)\n",
    "    \n",
    "    df['tweet_lang']=tweet_lang\n",
    "    df['tweet_en']=tweet_en\n",
    "    df['tweet_pt']=tweet_pt\n",
    "        \n",
    "    # save in new folder\n",
    "    #df.to_csv('Dataset_Twitter_Clean_02\\\\'+company[0]+'.csv',index=None)\n",
    "    df.to_pickle('Dataset_Twitter_Clean_02/'+company[0]+'.pkl')\n",
    "        \n",
    "    # timing (end)\n",
    "    t_sec = round(time.time() - start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60) \n",
    "    print('Time passed: {}hour:{}min:{}sec'.format(t_hour,t_min,t_sec))\n",
    "    print('------------------')\n",
    "    \n",
    "    \n",
    "# alternatives packages -> overall, unavailable for large datasets\n",
    "# alternative 1\n",
    "#import spacy\n",
    "#from spacy_langdetect import LanguageDetector\n",
    "\n",
    "#def language_spacy(tweet):\n",
    "#    nlp = spacy.load(\"en\")\n",
    "#    nlp.add_pipe(LanguageDetector(), name=\"language_detector\", last=True)\n",
    "#    doc = nlp(tweet)\n",
    "#    return doc._.language['language']\n",
    "\n",
    "#tweet = \"hello my dear friend\"\n",
    "#print(language_spacy(tweet))\n",
    "\n",
    "# alternative 2\n",
    "#from textblob import TextBlob\n",
    "\n",
    "#tweet = \"hello dear friend\"\n",
    "#print(TextBlob(tweet).detect_language())\n",
    "\n",
    "# alternative 3\n",
    "#from googletrans import Translator\n",
    "\n",
    "#translator=Translator()\n",
    "\n",
    "#tweet = \"hello my dear friend\"\n",
    "#detected = translator.detect(tweet)\n",
    "#langu = detected.lang\n",
    "#proba = detected.confidence\n",
    "#print(langu,proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TweetFilter(company):\n",
    "    \n",
    "    # timing (start)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # open and read file\n",
    "    filename='Dataset_Twitter_Clean_02/'+company[0]+'.pkl'\n",
    "    df = pd.read_pickle(filename)\n",
    "    print(company[0])\n",
    "    \n",
    "    # predominant language\n",
    "    print('Filter: language == {}'.format(company[0][:2]))\n",
    "    print(df.shape)\n",
    "    \n",
    "    if company[0][:2]=='br':\n",
    "        df=df[df.tweet_pt==1]\n",
    "    else:\n",
    "        df=df[df.tweet_en==1]\n",
    "        \n",
    "    print(df.shape)\n",
    "    \n",
    "    # non informative tweets\n",
    "    print('Filter: at least one stem')\n",
    "    print(df.shape)\n",
    "    df = df[df.count_stems>0]\n",
    "    print(df.shape)\n",
    "    \n",
    "    # users that look like spam\n",
    "    print('Filter: followers >= 40 (not spam-like)')\n",
    "    print(df.shape)\n",
    "    df = df[df.user_followers>=40]\n",
    "    print(df.shape)    \n",
    "    \n",
    "    # save in new folder\n",
    "    df.to_pickle('Dataset_Twitter_Clean_03/'+company[0]+'.pkl')\n",
    "    \n",
    "    # timing (end)\n",
    "    t_sec = round(time.time() - start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60) \n",
    "    print('Time passed: {}hour:{}min:{}sec'.format(t_hour,t_min,t_sec))\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Pre-Cleaner\n",
      "us_wsj\n",
      "(1962643, 5)\n",
      "(1962643, 5)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "(\"'float' object has no attribute 'lower'\", 'occurred at index 731864')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-bb4d253b5990>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tweet Pre-Cleaner'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcompany\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmy_companies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mTweetPreCleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompany\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# -> save in clean_01\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-994975bf4372>\u001b[0m in \u001b[0;36mTweetPreCleaner\u001b[1;34m(company)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# this will be useful later on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# save in new folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)\u001b[0m\n\u001b[0;32m   6911\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6912\u001b[0m         )\n\u001b[1;32m-> 6913\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6915\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;31m# compute the result using the series generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;31m# wrap results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-994975bf4372>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# this will be useful later on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# save in new folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: (\"'float' object has no attribute 'lower'\", 'occurred at index 731864')"
     ]
    }
   ],
   "source": [
    "my_companies = [['br_embraer','@EMBRAER','#EMBRAER'],\n",
    "                ['br_americanas','@LOJASAMERICANAS','#LOJASAMERICANAS'],\n",
    "                ['br_pontofrio','@PONTOFRIO','#PONTOFRIO'],\n",
    "                ['br_petrobras','@PETROBRAS','#PETROBRAS'],\n",
    "                ['br_bradesco','@BRADESCO','#BRADESCO'],\n",
    "                ['br_itau','@ITAU','#ITAU'],\n",
    "                ['br_renner','@LOJAS_RENNER','#RENNER'],\n",
    "                ['br_gol','@VOEGOLOFICIAL','#VOEGOL'],\n",
    "                ['br_magazineluiza','@MAGAZINELUIZA','#MAGALU'],\n",
    "                ['br_valor','@VALORECONOMICO','#VALORECONOMICO'],\n",
    "                ['us_abercrombie','@ABERCROMBIE','#ABERCROMBIE'],\n",
    "                ['us_boeing','@BOEING','#BOEING'],\n",
    "                ['us_beyondmeat','@BEYONDMEAT','#BEYONDMEAT'],\n",
    "                ['us_morganstanley','@MORGANSTANLEY','#MORGANSTANLEY'],\n",
    "                ['us_jpmorgan','@JPMORGAN','#JPMORGAN'],\n",
    "                ['us_exxonmobil','@EXXONMOBIL','#EXXON'],\n",
    "                ['us_americanair','@AMERICANAIR','#AMERICANAIRLINES'],\n",
    "                ['us_cocacola','@COCACOLA','#COCACOLA'],\n",
    "                ['us_tesla','@TESLA','#TESLA'],\n",
    "                ['us_wsj','@WSJ','#WSJ']]\n",
    "\n",
    "my_companies.append(['br_PortugueseTweets','@PORTUGUESE_TWEETS','#PORTUGUESE_TWEETS'])\n",
    "my_companies.append(['us_EnglishTweets','@ENGLISH_TWEETS','#ENGLISH_TWEETS'])\n",
    "\n",
    "################################################\n",
    "print('Tweet Pre-Cleaner')\n",
    "for company in my_companies:\n",
    "    TweetPreCleaner(company)\n",
    "\n",
    "# -> save in clean_01\n",
    "\n",
    "print('Tweet Cleaner')\n",
    "for company in my_companies:\n",
    "    TweetCleaner(company)\n",
    "\n",
    "# -> save in clean_01\n",
    "\n",
    "print('Tweet Stemmer')\n",
    "for company in my_companies:\n",
    "    TweetStemmer(company)\n",
    "    \n",
    "# -> save in clean_01\n",
    "\n",
    "print('Tweet Language')\n",
    "for company in my_companies:\n",
    "    TweetLanguage(company)\n",
    "\n",
    "# -> save in clean_02\n",
    "\n",
    "##### ANALYSIS OF 2.2 IS RUN HERE #####\n",
    "\n",
    "print('Tweet Filter')\n",
    "for company in my_companies:\n",
    "    TweetFilter(company)\n",
    "\n",
    "# -> save in clean_03\n",
    "\n",
    "##### GRAPHS OF 2.3 ARE GENERATED HERE #####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
