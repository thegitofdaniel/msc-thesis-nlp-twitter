{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Financial Model (Intraday)\n",
    "Daniel Ruiz, MSc in Data Science and Business Analytics (DSBA), Bocconi University\n",
    "\n",
    "Reference codes (alphabetically):\n",
    "- https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python\n",
    "- https://machinelearningmastery.com/how-to-develop-convolutional-neural-network-models-for-time-series-forecasting/\n",
    "- https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/\n",
    "- https://pythonprogramming.net/convolutional-neural-network-deep-learning-python-tensorflow-keras/\n",
    "- https://stackoverflow.com/questions/32419510/how-to-get-reproducible-results-in-keras\n",
    "- https://towardsdatascience.com/how-to-use-convolutional-neural-networks-for-time-series-classification-56b1b0a07a57\n",
    "\n",
    "\n",
    "## 3.1. Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from numpy import array, hstack, vstack\n",
    "\n",
    "# graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# neural networks\n",
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame(['hello'],columns=['my'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2  Loading and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(company, print_steps=False):\n",
    "\n",
    "    if print_steps:\n",
    "        print(company)\n",
    "\n",
    "    # 1) ---------------------------------------\n",
    "    # load twitter data\n",
    "    df_twitter = pd.read_pickle('Dataset_TS_02/'+company+'.pkl')\n",
    "\n",
    "    # get variables\n",
    "    df_twitter['final_neg']=1-df_twitter['final_pos']\n",
    "    sup1 = df_twitter.groupby(['block_15'])['final_pos','final_neg'].sum()\n",
    "    sup2 = df_twitter.groupby(['block_15'])['final_pos'].mean()\n",
    "    df_twitter = sup1.merge(sup2,on='block_15', how='left')\n",
    "    df_twitter.columns = [['count_pos','count_neg','avg_pos']]\n",
    "\n",
    "    if print_steps:\n",
    "        print('Twitter DF (block 15)', df_twitter.shape)\n",
    "        print('1-ok!')\n",
    "        \n",
    "    # 2) ---------------------------------------\n",
    "    # load finacial data\n",
    "    df_stocks = pd.read_pickle('Dataset_TS_FIN_01/'+company+'.pkl')\n",
    "    df_stocks = df_stocks.sort_values(['datetime'])\n",
    "\n",
    "    # log\n",
    "    df_stocks['high'] = np.log(df_stocks['high'])\n",
    "    df_stocks['low'] = np.log(df_stocks['low'])\n",
    "    df_stocks['open'] = np.log(df_stocks['open'])\n",
    "    df_stocks['close'] = np.log(df_stocks['close'])\n",
    "    \n",
    "    #XXX\n",
    "    #df_stocks['volume'] = [int(vol.replace(\".\",\"\")) for vol in df_stocks.volume]\n",
    "    df_stocks['volume'] = np.log(df_stocks['volume'])\n",
    "\n",
    "    # Tweets [14:15:00,14:29:59] -> Prediction Open of [14:30:00]\n",
    "    df_stocks['delta_open'] = df_stocks['open'].shift(-1)-df_stocks['open']\n",
    "    df_stocks['delta_open_pos']=df_stocks['delta_open']>=0\n",
    "    \n",
    "    # Volume [14:15:00,14:29:59] -> 14:29:59 - 14:14:59\n",
    "    df_stocks['delta_close'] = df_stocks['close']-df_stocks['close'].shift(1)\n",
    "    df_stocks['delta_volume'] = df_stocks['volume']-df_stocks['volume'].shift(1)\n",
    "    #df_stocks['delta_close_pos']=df_stocks['delta_open']>=0\n",
    "\n",
    "    # cleaning first and last block of the day\n",
    "    df_stocks = df_stocks[df_stocks.date_adj==df_stocks.date_adj.shift(-1)]\n",
    "    df_stocks = df_stocks[df_stocks.date_adj==df_stocks.date_adj.shift(1)]\n",
    "    \n",
    "    if print_steps:\n",
    "        print('Stocks DF (block 15)', df_stocks.shape)\n",
    "        print('2-ok!')\n",
    "\n",
    "    # 3) ---------------------------------------\n",
    "    # merge the data\n",
    "    XY = df_stocks.merge(df_twitter, on='block_15', how='left')\n",
    "    XY = XY.rename(columns={('count_pos',): 'count_pos',\n",
    "                            ('count_neg',): 'count_neg',\n",
    "                            ('delta_open_pos',): 'delta_open_pos',\n",
    "                            ('avg_pos',): 'avg_pos',\n",
    "                            ('delta_volume',): 'delta_volume',\n",
    "                            ('delta_close',): 'delta_close'},)\n",
    "    \n",
    "    if print_steps:\n",
    "        print(XY.columns)\n",
    "        \n",
    "    # filtering data\n",
    "    XY = XY[XY.block_on==True]\n",
    "    used_variables = ['count_pos','count_neg','avg_pos','delta_close','delta_volume']\n",
    "    n_X_vars = len(used_variables)\n",
    "    XY = XY[['date_adj','block_15','delta_open_pos']+used_variables]\n",
    "\n",
    "    # sort by datetime\n",
    "    XY = XY.sort_values(['block_15'])\n",
    "    XY = XY.reset_index(drop=True)\n",
    "\n",
    "    # filling in NAs: n_tweets = 0, avg_pos = avg, demeaning\n",
    "    try:\n",
    "        XY['count_pos'] = XY['count_pos'].fillna(0)\n",
    "        XY['count_neg'] = XY['count_neg'].fillna(0)\n",
    "        XY['avg_pos'] = XY['avg_pos'].fillna(XY['avg_pos'].mean())- XY['avg_pos'].mean()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if print_steps:\n",
    "        print('3-ok!')\n",
    "\n",
    "    # 4) ---------------------------------------\n",
    "    # convert to NN readable format\n",
    "    \n",
    "    \n",
    "    # split a multivariate sequence into samples\n",
    "    def split_sequences(sequences, n_steps):\n",
    "        X, y = list(), list()\n",
    "        for i in range(len(sequences)):\n",
    "            # find the end of this pattern\n",
    "            end_ix = i + n_steps\n",
    "            # check if we are beyond the dataset\n",
    "            if end_ix > len(sequences):\n",
    "                break\n",
    "            # gather input and output parts of the pattern\n",
    "            seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "            X.append(seq_x)\n",
    "            y.append(seq_y)\n",
    "        return array(X), array(y)\n",
    "    \n",
    "    # past n_teps blocks of 15 minutes\n",
    "    n_steps = 3\n",
    "    n_features = 1\n",
    "\n",
    "    X_all = np.empty((0,n_steps,n_X_vars), int)\n",
    "    y_all = np.empty((0), int)\n",
    "\n",
    "    # for all dates\n",
    "    sup_dates = XY.date_adj.unique().tolist()\n",
    "    for sup_date in sup_dates:\n",
    "        sup_XY = XY[XY.date_adj==sup_date].T.values.tolist()\n",
    "\n",
    "        # x\n",
    "        seqs = []\n",
    "        for i in range(n_X_vars):\n",
    "            seq = array(sup_XY[3+i])\n",
    "            seq = seq.reshape((len(seq), 1))\n",
    "            seqs.append(seq)\n",
    "        # y\n",
    "        seq = array(sup_XY[2])\n",
    "        seq = seq.reshape((len(seq), 1))\n",
    "        seqs.append(seq)\n",
    "\n",
    "        # convert into input/output\n",
    "        dataset = hstack(tuple(seqs))\n",
    "        X, y = split_sequences(dataset, n_steps)\n",
    "\n",
    "        # stack\n",
    "        X_all = vstack((X_all,X))\n",
    "        y_all = hstack((y_all,y))\n",
    "    \n",
    "    if print_steps:\n",
    "        print('4-ok!')\n",
    "    \n",
    "    # 5) ---------------------------------------\n",
    "    \n",
    "    # separate input data (n_features = time-series per head)\n",
    "    X=[]\n",
    "    for i in range(n_X_vars):\n",
    "        X.append(X_all[:, :, i].reshape(X_all.shape[0], X_all.shape[1], n_features))\n",
    "\n",
    "    # Train, Test, Valid = 60, 20, 20\n",
    "    sup = int(np.ceil(len(y_all)*0.2))\n",
    "    X_train, X_valid, X_test = [], [], []\n",
    "    \n",
    "    for i in range(n_X_vars):\n",
    "        X_train.append(X[i][:len(y_all)-2*sup])\n",
    "        X_valid.append(X[i][len(y_all)-2*sup:len(y_all)-sup])\n",
    "        X_test.append(X[i][len(y_all)-sup:len(y_all)])\n",
    "\n",
    "    # y\n",
    "    y_train = y_all[:len(y_all)-2*sup]\n",
    "    y_valid = y_all[len(y_all)-2*sup:len(y_all)-sup]\n",
    "    y_test = y_all[len(y_all)-sup:len(y_all)]\n",
    "    \n",
    "    if print_steps:\n",
    "        print('5-ok!')\n",
    "    \n",
    "    return[[X_train,X_valid,X_test],[y_train,y_valid,y_test]]\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "def prepare_model(n_variables,n_steps=3,n_features=1,num_classes=2,print_steps=False):\n",
    "    \n",
    "    def make_cnn(n_steps=n_steps,n_features=n_features):\n",
    "        visible = Input(shape=(n_steps, n_features))\n",
    "        cnn = Conv1D(filters=16, kernel_size=2, activation='relu')(visible)\n",
    "        cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "        cnn = Dropout(0.25)(cnn)\n",
    "        cnn = Flatten()(cnn)\n",
    "        return visible, cnn\n",
    "\n",
    "    # generate input models for each variable\n",
    "    visibles, cnns = [], []\n",
    "    for i in range(n_variables):\n",
    "    # add element\n",
    "        visibles.append([])\n",
    "        cnns.append([])\n",
    "        visibles[-1], cnns[-1] = make_cnn()\n",
    "\n",
    "    # merge input models\n",
    "    merge = concatenate(cnns)\n",
    "    dense = Dense(50, activation='relu')(merge)\n",
    "    dense = Dropout(0.3)(dense)\n",
    "    m_output = Dense(num_classes, activation='softmax')(dense)\n",
    "    model = Model(inputs=visibles, outputs=m_output)\n",
    "\n",
    "    # for continuous predictions\n",
    "    metricas = ['accuracy','mse','msle','mae', 'mape', 'cosine']\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=metricas)\n",
    "\n",
    "    #\n",
    "    #model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "    #              optimizer=keras.optimizers.Adam(),\n",
    "    #              metrics=['accuracy'])\n",
    "    \n",
    "    if print_steps:\n",
    "        # visualize the model\n",
    "        print(model.summary())\n",
    "        \n",
    "    return(model)\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "def save_graphs(model_train,company,folder='intraday_estimation/'):\n",
    "    \n",
    "    # performance in validation and test set\n",
    "    metricas = [i for i in list(model_train.history.keys()) if i[:4]!='val_']\n",
    "    for metrica in metricas:\n",
    "        met = model_train.history[metrica]\n",
    "        val_met = model_train.history['val_'+metrica]\n",
    "        epochs = range(len(met))\n",
    "        plt.figure(figsize=(15,5))\n",
    "        sns.set(font_scale=1.5)\n",
    "        plt.plot(epochs, met, 'b', c='orange', label='Training '+metrica,lw=1)\n",
    "        plt.plot(epochs, val_met, 'b', c='green', label='Validation '+metrica,lw=1)\n",
    "        plt.title('Training and validation '+metrica)\n",
    "        plt.legend(loc='center left')\n",
    "        plt.savefig(folder+company+'_'+metrica+'.png')\n",
    "        plt.close()\n",
    "        \n",
    "#------------------------------------------------------------------------\n",
    "def find_cut(model,X,y):\n",
    "                \n",
    "    predictions = model.predict(X)[:,1]\n",
    "\n",
    "    total = len(predictions)\n",
    "    max_acc=0\n",
    "    min_acc=1\n",
    "\n",
    "    # loop\n",
    "    sup_min = max(int(min(predictions)*100)+1,33)\n",
    "    sup_max = min(int(max(predictions)*100)-1,67)\n",
    "    best_cut = sup_min\n",
    "\n",
    "    for cut in range(sup_min,sup_max): \n",
    "\n",
    "        hits = sum((predictions>=(cut/100))==y)\n",
    "\n",
    "        acc = hits/total\n",
    "\n",
    "        if acc >= max_acc:\n",
    "            best_cut = cut\n",
    "            max_acc = acc\n",
    "\n",
    "        if acc <= min_acc:\n",
    "            worst_cut = cut\n",
    "            min_acc = acc\n",
    "            \n",
    "    return max_acc, best_cut, min_acc, worst_cut\n",
    "        \n",
    "#------------------------------------------------------------------------\n",
    "def return_confusion(model, X_test, y_test, print_steps=False, cut=50):\n",
    "\n",
    "    # predicted class = most likely (MECE)\n",
    "    predicted_classes = array(model.predict(X_test)[:,1]>=(cut/100),dtype=int)\n",
    "\n",
    "    # overall performance:\n",
    "    if print_steps:\n",
    "        print(\"Of {} observations tested:\".format(len(predicted_classes)))\n",
    "        correct = np.where(predicted_classes==y_test)[0]\n",
    "        print(\"- Classified {} correctly\".format(len(correct)))\n",
    "        incorrect = np.where(predicted_classes!=y_test)[0]\n",
    "        print(\"- Classified {} incorrectly\".format(len(incorrect)))\n",
    "\n",
    "    # confusion matrix\n",
    "    # 0,0 = TN = True Negatives\n",
    "    # 0,1 = FP = False Positives\n",
    "    # 1,0 = FN = False Negatives\n",
    "    # 0,1 = TP = True Positives\n",
    "    cm = confusion_matrix(y_test,predicted_classes)\n",
    "    TN, FP, FN, TP = cm.flatten()\n",
    "    total = TN+FP+FN+TP\n",
    "\n",
    "    # class 1\n",
    "    prec1 = TP / (TP+FP)\n",
    "    reca1 = TP / (TP+FN)\n",
    "    fone1 = 2*(prec1*reca1)/(prec1+reca1)\n",
    "    # class 0\n",
    "    prec0 = TN / (TN+FN)\n",
    "    reca0 = TN / (TN+FP)\n",
    "    fone0 = 2*(prec0*reca0)/(prec0+reca0)\n",
    "\n",
    "    # global / weighted\n",
    "    accuw = TP/total +TN/total\n",
    "    precw = prec0*(TN+FP)/(total) + prec1*(TP+FN)/(total)\n",
    "    recaw = reca0*(TN+FP)/(total) + reca1*(TP+FN)/(total)\n",
    "    fonew = fone0*(TN+FP)/(total) + fone1*(TP+FN)/(total)\n",
    "\n",
    "    # list\n",
    "    sup = [company, TN, FP, FN, TP, prec1, reca1, fone1, prec0, reca0, fone0, precw, recaw, fonew, accuw]\n",
    "\n",
    "    return sup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2. Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'log'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e8c0c6ef2d90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# prepare data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompany\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprint_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m# unpacking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-eab714030d14>\u001b[0m in \u001b[0;36mprepare_data\u001b[1;34m(company, print_steps)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m#XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m#df_stocks['volume'] = [int(vol.replace(\".\",\"\")) for vol in df_stocks.volume]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mdf_stocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'volume'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_stocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'volume'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# Tweets [14:15:00,14:29:59] -> Prediction Open of [14:30:00]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__array_ufunc__\u001b[1;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 853\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mufunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    854\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m             \u001b[1;31m# we require names to be hashable, right?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'log'"
     ]
    }
   ],
   "source": [
    "# set seeds\n",
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# companies\n",
    "my_companies = ['br_embraer',                \n",
    "                'br_americanas',\n",
    "                'br_pontofrio',\n",
    "                'br_petrobras',\n",
    "                'br_bradesco',\n",
    "                'br_renner',\n",
    "                'br_gol',\n",
    "                'br_magazineluiza',\n",
    "                'br_itau',\n",
    "                'us_abercrombie',\n",
    "                'us_boeing',\n",
    "                'us_beyondmeat',\n",
    "                'us_morganstanley',\n",
    "                'us_jpmorgan',\n",
    "                'us_exxonmobil',\n",
    "                'us_americanair',\n",
    "                'us_cocacola',\n",
    "                'us_tesla',\n",
    "                'us_wsj']\n",
    "\n",
    "\n",
    "folder='intraday_estimation/'\n",
    "with open(folder+'performance_intraday_'+time.strftime('%Y-%m-%d_%H-%M',time.gmtime())+'.csv', 'w', encoding=\"utf-8\") as csvFile:\n",
    "\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    for company in my_companies:\n",
    "\n",
    "        # prepare data\n",
    "        X, y = prepare_data(company,print_steps=False)\n",
    "\n",
    "        # unpacking\n",
    "        X_train, X_valid, X_test = X\n",
    "        y_train, y_valid, y_test = y\n",
    "\n",
    "        # one hot enconding\n",
    "        y_train_one_hot = to_categorical(y_train)\n",
    "        y_valid_one_hot = to_categorical(y_valid)\n",
    "        y_test_one_hot = to_categorical(y_test)\n",
    "\n",
    "        # 1st input model -> count_pos\n",
    "        # 2nd input model -> count_neg\n",
    "        # 3rd input model -> avg_pos\n",
    "        # 4th input model -> delta volume (lag)\n",
    "\n",
    "        model = prepare_model(n_variables=len(X_train))\n",
    "\n",
    "        # fit model (verbose = print steps)\n",
    "        model_train = model.fit(X_train,\n",
    "                                y_train_one_hot,\n",
    "                                epochs=100,\n",
    "                                verbose=0,\n",
    "                                validation_data=(X_valid, y_valid_one_hot))\n",
    "\n",
    "        save_graphs(model_train,company,folder)\n",
    "\n",
    "        # test set final loss and accuracy\n",
    "        test_eval = model.evaluate(X_test,y_test_one_hot,verbose=0)\n",
    "\n",
    "        # hit an miss\n",
    "        #max_acc, best_cut, min_acc, worst_cut = find_cut(model,X_valid,y_valid)\n",
    "        #my_cut=best_cut        \n",
    "        my_cut=50\n",
    "        \n",
    "        sup = return_confusion(model,\n",
    "                               X_test,\n",
    "                               y_test,\n",
    "                               print_steps=False,\n",
    "                               cut=my_cut)\n",
    "        csvWriter.writerow(sup)\n",
    "        \n",
    "        # print\n",
    "        print(company)\n",
    "        print('Test loss:', test_eval[0])\n",
    "        print('Test accuracy:', test_eval[1])\n",
    "        print('my cut:',my_cut)\n",
    "        print(sup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
