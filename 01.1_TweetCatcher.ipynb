{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extracting tweets\n",
    "\n",
    "Daniel Ruiz, MSc in Data Science and Business Analytics (DSBA), Bocconi University\n",
    "\n",
    "Reference codes (alphabetically):\n",
    "- https://chatbotslife.com/twitter-data-mining-a-guide-to-big-data-analytics-using-python-4efc8ccfa219\n",
    "- https://developer.twitter.com/en/docs\n",
    "- http://docs.tweepy.org/en/v3.8.0/cursor_tutorial.html\n",
    "- https://gist.github.com/vickyqian/f70e9ab3910c7c290d9d715491cde44c\n",
    "- https://towardsdatascience.com/creating-the-twitter-sentiment-analysis-program-in-python-with-naive-bayes-classification-672e5589a7ed\n",
    "- https://towardsdatascience.com/extracting-twitter-data-pre-processing-and-sentiment-analysis-using-python-3-0-7192bd8b47cf\n",
    "- https://towardsdatascience.com/tweepy-for-beginners-24baf21f2c25\n",
    "\n",
    "## 1.1. Activating the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File with list of: 0 Consumer key, 1 Consumer secret, 2 Access Token, and 3 Acces token secret\n",
    "\n",
    "auths = json.load(open('au.txt'))\n",
    "auth = tweepy.OAuthHandler(auths[0], auths[1])\n",
    "auth.set_access_token(auths[2], auths[3])\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Rest API : Tweets sent before the query started (i.e. historical)\n",
    "# Streaming API: tweets sent after the query started (i.e. real-time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Running a query"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Scrap tweets\n",
    "\n",
    "# Original source: https://gist.github.com/vickyqian/f70e9ab3910c7c290d9d715491cde44c\n",
    "\n",
    "# tweet_mode = 'extended' -> above 140 chars\n",
    "# geocode = ? -> few have it\n",
    "# include_rts=False\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "                       q='@MICROSOFT OR #MICROSOFT',\n",
    "                       tweet_mode='extended',\n",
    "                       count=100,\n",
    "                       since='2020-01-01',\n",
    "                       until='2020-01-14',\n",
    "                       wait_on_rate_limit=True).items(100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write down tweets\n",
    "\n",
    "for tweet in tweets:\n",
    "    \n",
    "    # Open/Create a file to append data\n",
    "    with open('microsoft.csv', 'a', encoding=\"utf-8\") as csvFile:\n",
    "\n",
    "        # Use csv writer\n",
    "        csvWriter = csv.writer(csvFile)\n",
    "\n",
    "        # other arguments include:\n",
    "        # tweet.retweet_count, .favorite_count, .entities\n",
    "        # tweet.user.screen_name, .description, .location\n",
    "\n",
    "        csvWriter.writerow([[tweet.full_text],\n",
    "                            [tweet.id_str],\n",
    "                            [tweet.created_at]])\n",
    "    \n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Creating a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tweets(api, sup, start_date='2020-01-01', end_date='2020-01-14'):\n",
    "    \n",
    "    # start_date = 'YYYY-MM-DD' (inclusive)\n",
    "    # end_date = 'YYYY-MM-DD' (exclusive)\n",
    "    # filename= \"XXX.csv\"\n",
    "\n",
    "    tweets = tweepy.Cursor(api.search,\n",
    "                       q=sup[1],\n",
    "                       tweet_mode='extended',\n",
    "                       since=start_date,\n",
    "                       until=end_date,\n",
    "                       wait_on_rate_limit=True).items()\n",
    "    \n",
    "    # Display\n",
    "    print('searching:', sup[1])\n",
    "    filename='Dataset_Twitter\\\\'+sup[0]+'.csv'\n",
    "    print('folder:', filename)\n",
    "    \n",
    "    for tweet in tweets:\n",
    "\n",
    "        # Open/Create a file to append data\n",
    "        \n",
    "        with open(filename, 'a', encoding=\"utf-8\") as csvFile:\n",
    "\n",
    "            # Use csv writer\n",
    "            csvWriter = csv.writer(csvFile)\n",
    "\n",
    "            # other arguments include:\n",
    "            # tweet.retweet_count, .favorite_count, .entities\n",
    "            # tweet.user.description, .location\n",
    "\n",
    "            csvWriter.writerow([tweet.full_text.replace(\",\",\" \"),\n",
    "                                tweet.id_str,\n",
    "                                tweet.created_at,\n",
    "                                tweet.user.screen_name,\n",
    "                                tweet.user.followers_count])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Running the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_companies = [['br_embraer','@EMBRAER OR #EMBRAER'], #0\n",
    "                ['br_americanas','@LOJASAMERICANAS OR #LOJASAMERICANAS'], #1\n",
    "                ['br_pontofrio','@PONTOFRIO OR #PONTOFRIO'], #2\n",
    "                ['br_petrobras','@PETROBRAS OR #PETROBRAS'], #3\n",
    "                ['br_bradesco','@BRADESCO OR #BRADESCO'], #4\n",
    "                ['br_renner','@LOJAS_RENNER OR #RENNER'], #5\n",
    "                ['br_gol','@VOEGOLOFICIAL OR #VOEGOL'], #6\n",
    "                ['br_magazineluiza','@MAGAZINELUIZA OR #MAGALU'], #7\n",
    "                ['br_itau','@ITAU OR #ITAU'],\n",
    "                ['br_valor','@VALORECONOMICO OR #VALORECONOMICO'], #8\n",
    "                ['us_abercrombie','@ABERCROMBIE OR #ABERCROMBIE'], #9\n",
    "                ['us_boeing','@BOEING OR #BOEING'], #10\n",
    "                ['us_beyondmeat','@BEYONDMEAT OR #BEYONDMEAT'], #11\n",
    "                ['us_morganstanley','@MORGANSTANLEY OR #MORGANSTANLEY'], #12\n",
    "                ['us_jpmorgan', '@JPMORGAN OR #JPMORGAN'], #13\n",
    "                ['us_exxonmobil','@EXXONMOBIL OR #EXXON'], #14\n",
    "                ['us_americanair','@AMERICANAIR OR #AMERICANAIRLINES'], #15\n",
    "                ['us_cocacola','@COCACOLA OR #COCACOLA'], #16\n",
    "                ['us_tesla','@TESLA OR #TESLA'], #17\n",
    "                ['us_wsj','@WSJ OR #WSJ']] #18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['br_embraer', '@EMBRAER OR #EMBRAER']\n",
      "searching: @EMBRAER OR #EMBRAER\n",
      "folder: Dataset_Twitter\\br_embraer.csv\n",
      "--------------\n",
      "1 ['br_americanas', '@LOJASAMERICANAS OR #LOJASAMERICANAS']\n",
      "searching: @LOJASAMERICANAS OR #LOJASAMERICANAS\n",
      "folder: Dataset_Twitter\\br_americanas.csv\n",
      "--------------\n",
      "2 ['br_pontofrio', '@PONTOFRIO OR #PONTOFRIO']\n",
      "searching: @PONTOFRIO OR #PONTOFRIO\n",
      "folder: Dataset_Twitter\\br_pontofrio.csv\n",
      "--------------\n",
      "3 ['br_petrobras', '@PETROBRAS OR #PETROBRAS']\n",
      "searching: @PETROBRAS OR #PETROBRAS\n",
      "folder: Dataset_Twitter\\br_petrobras.csv\n",
      "--------------\n",
      "4 ['br_bradesco', '@BRADESCO OR #BRADESCO']\n",
      "searching: @BRADESCO OR #BRADESCO\n",
      "folder: Dataset_Twitter\\br_bradesco.csv\n",
      "--------------\n",
      "5 ['br_renner', '@LOJAS_RENNER OR #RENNER']\n",
      "searching: @LOJAS_RENNER OR #RENNER\n",
      "folder: Dataset_Twitter\\br_renner.csv\n",
      "--------------\n",
      "6 ['br_gol', '@VOEGOLOFICIAL OR #VOEGOL']\n",
      "searching: @VOEGOLOFICIAL OR #VOEGOL\n",
      "folder: Dataset_Twitter\\br_gol.csv\n",
      "--------------\n",
      "7 ['br_magazineluiza', '@MAGAZINELUIZA OR #MAGALU']\n",
      "searching: @MAGAZINELUIZA OR #MAGALU\n",
      "folder: Dataset_Twitter\\br_magazineluiza.csv\n",
      "--------------\n",
      "8 ['br_itau', '@ITAU OR #ITAU']\n",
      "searching: @ITAU OR #ITAU\n",
      "folder: Dataset_Twitter\\br_itau.csv\n",
      "--------------\n",
      "9 ['br_valor', '@VALORECONOMICO OR #VALORECONOMICO']\n",
      "searching: @VALORECONOMICO OR #VALORECONOMICO\n",
      "folder: Dataset_Twitter\\br_valor.csv\n",
      "--------------\n",
      "10 ['us_abercrombie', '@ABERCROMBIE OR #ABERCROMBIE']\n",
      "searching: @ABERCROMBIE OR #ABERCROMBIE\n",
      "folder: Dataset_Twitter\\us_abercrombie.csv\n",
      "--------------\n",
      "11 ['us_boeing', '@BOEING OR #BOEING']\n",
      "searching: @BOEING OR #BOEING\n",
      "folder: Dataset_Twitter\\us_boeing.csv\n",
      "--------------\n",
      "12 ['us_beyondmeat', '@BEYONDMEAT OR #BEYONDMEAT']\n",
      "searching: @BEYONDMEAT OR #BEYONDMEAT\n",
      "folder: Dataset_Twitter\\us_beyondmeat.csv\n",
      "--------------\n",
      "13 ['us_morganstanley', '@MORGANSTANLEY OR #MORGANSTANLEY']\n",
      "searching: @MORGANSTANLEY OR #MORGANSTANLEY\n",
      "folder: Dataset_Twitter\\us_morganstanley.csv\n",
      "--------------\n",
      "14 ['us_jpmorgan', '@JPMORGAN OR #JPMORGAN']\n",
      "searching: @JPMORGAN OR #JPMORGAN\n",
      "folder: Dataset_Twitter\\us_jpmorgan.csv\n",
      "--------------\n",
      "15 ['us_exxonmobil', '@EXXONMOBIL OR #EXXON']\n",
      "searching: @EXXONMOBIL OR #EXXON\n",
      "folder: Dataset_Twitter\\us_exxonmobil.csv\n",
      "--------------\n",
      "16 ['us_americanair', '@AMERICANAIR OR #AMERICANAIRLINES']\n",
      "searching: @AMERICANAIR OR #AMERICANAIRLINES\n",
      "folder: Dataset_Twitter\\us_americanair.csv\n",
      "--------------\n",
      "17 ['us_cocacola', '@COCACOLA OR #COCACOLA']\n",
      "searching: @COCACOLA OR #COCACOLA\n",
      "folder: Dataset_Twitter\\us_cocacola.csv\n",
      "--------------\n",
      "18 ['us_tesla', '@TESLA OR #TESLA']\n",
      "searching: @TESLA OR #TESLA\n",
      "folder: Dataset_Twitter\\us_tesla.csv\n",
      "--------------\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "for company in my_companies[i:19]:\n",
    "    print(i, company)\n",
    "    write_tweets(api,sup=company,start_date='2020-04-24',end_date='2020-04-25')\n",
    "    i+=1\n",
    "    print('--------------')\n",
    "    \n",
    "print('done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
