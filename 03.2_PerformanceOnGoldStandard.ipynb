{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classifier Performance on Gold-Standard\n",
    "\n",
    "Daniel Ruiz, MSc in Data Science and Business Analytics (DSBA), Bocconi University\n",
    "\n",
    "Reference codes (alphabetically):\n",
    "\n",
    "## 3.1. Loading packages and creating folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import time\n",
    "\n",
    "# classification\n",
    "from BOW import *\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Gold-standard creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------\n",
    "def NoisyLabelExtractor(company):\n",
    "    \n",
    "    # extracts info and pre-process text\n",
    "    \n",
    "    # timing (start)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # open and read file (company)\n",
    "    filename_sup='Dataset_Twitter_Clean_03/'+company[0]+'.pkl'\n",
    "    df_sup = pd.read_pickle(filename_sup)\n",
    "    print(company[0])\n",
    "    \n",
    "    df_sup = df_sup[['snowball_stems','has_happy','has_sad']]\n",
    " \n",
    "    # open and read file ()\n",
    "    filename='Dataset_NoisyLabels/'+company[0][:2]+'_NoisyLabels.pkl'\n",
    "    df = pd.read_pickle(filename)\n",
    "    \n",
    "    df = df.append(df_sup, ignore_index = True)\n",
    "\n",
    "    # save\n",
    "    df.to_pickle(filename)\n",
    "    \n",
    "    # timing (end)\n",
    "    t_sec = round(time.time() - start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60) \n",
    "    print('Time passed: {}hour:{}min:{}sec'.format(t_hour,t_min,t_sec))\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------\n",
    "my_companies = [['br_embraer','@EMBRAER','#EMBRAER'],\n",
    "                ['br_americanas','@LOJASAMERICANAS','#LOJASAMERICANAS'],\n",
    "                ['br_pontofrio','@PONTOFRIO','#PONTOFRIO'],\n",
    "                ['br_petrobras','@PETROBRAS','#PETROBRAS'],\n",
    "                ['br_bradesco','@BRADESCO','#BRADESCO'],\n",
    "                ['br_itau','@ITAU','#ITAU'],\n",
    "                ['br_renner','@LOJAS_RENNER','#RENNER'],\n",
    "                ['br_gol','@VOEGOLOFICIAL','#VOEGOL'],\n",
    "                ['br_magazineluiza','@MAGAZINELUIZA','#MAGALU'],\n",
    "                ['br_valor','@VALORECONOMICO','#VALORECONOMICO'],\n",
    "                ['us_abercrombie','@ABERCROMBIE','#ABERCROMBIE'],\n",
    "                ['us_boeing','@BOEING','#BOEING'],\n",
    "                ['us_beyondmeat','@BEYONDMEAT','#BEYONDMEAT'],\n",
    "                ['us_morganstanley','@MORGANSTANLEY','#MORGANSTANLEY'],\n",
    "                ['us_jpmorgan','@JPMORGAN','#JPMORGAN'],\n",
    "                ['us_exxonmobil','@EXXONMOBIL','#EXXON'],\n",
    "                ['us_americanair','@AMERICANAIR','#AMERICANAIRLINES'],\n",
    "                ['us_cocacola','@COCACOLA','#COCACOLA'],\n",
    "                ['us_tesla','@TESLA','#TESLA'],\n",
    "                ['us_wsj','@WSJ','#WSJ']]\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "# create folders\n",
    "df = pd.DataFrame(columns = ['snowball_stems','has_happy','has_sad'])\n",
    "# english\n",
    "filename='Dataset_NoisyLabels/us_NoisyLabels.pkl'\n",
    "df.to_pickle(filename)\n",
    "# portuguese\n",
    "filename='Dataset_NoisyLabels/br_NoisyLabels.pkl'\n",
    "df.to_pickle(filename)\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "print('Getting Noisy Labels - PT')\n",
    "for company in my_companies:\n",
    "    if company[0][:2]=='br':\n",
    "        NoisyLabelExtractor(company)\n",
    "\n",
    "print('Getting Noisy Labels - EN')\n",
    "for company in my_companies:\n",
    "    if company[0][:2]=='us':\n",
    "        NoisyLabelExtractor(company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [['Classifiers/English/','Dataset_NoisyLabels/us_NoisyLabels.pkl'],\n",
    "         ['Classifiers/Portuguese/','Dataset_NoisyLabels/br_NoisyLabels.pkl']]\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    # name\n",
    "    folder=file[0]\n",
    "    filename=file[1]\n",
    "\n",
    "    # load data\n",
    "    df = pd.read_pickle(filename)\n",
    "    df = df[['snowball_stems','has_happy','has_sad']]\n",
    "\n",
    "    # Filter (excludent) noisy labels\n",
    "    print(df.shape)\n",
    "    df.has_happy = df.has_happy.clip(0,1)\n",
    "    df.has_sad = df.has_sad.clip(0,1)\n",
    "    df = df[df.has_happy != df.has_sad]\n",
    "    print(df.shape)\n",
    "\n",
    "    # filter columns\n",
    "    df = df[['snowball_stems','has_happy']]\n",
    "    df.columns = ['snowball_stems','sentiment_pos']\n",
    "\n",
    "    # balanced ?\n",
    "    print('balanced?')\n",
    "    print('Sentiments:',df.sentiment_pos.unique())\n",
    "    print('Positive (1):',sum(df.sentiment_pos==1))\n",
    "    print('Negative (0):',sum(df.sentiment_pos==0))\n",
    "    if sum(df.sentiment_pos==1) != sum(df.sentiment_pos==0):\n",
    "        sup = min(sum(df.sentiment_pos==1),sum(df.sentiment_pos==0))\n",
    "        df_pos = df[df.sentiment_pos==1].sample(sup)\n",
    "        df_neg = df[df.sentiment_pos==0].sample(sup)\n",
    "        df = pd.concat([df_pos,df_neg]).reset_index(drop=True)\n",
    "        print('balanced')    \n",
    "        print('Positive (1):',sum(df.sentiment_pos==1))\n",
    "        print('Negative (0):',sum(df.sentiment_pos==0))\n",
    "\n",
    "    # bag_of_words\n",
    "    lfeats = label_feats_from_corpus(df.snowball_stems,df.sentiment_pos,bag_of_words)\n",
    "    train_feats, test_feats = split_label_feats(lfeats, split=0.0)\n",
    "    print('Test set size:',len(test_feats))\n",
    "    X_test = [test_feat[0] for test_feat in test_feats]\n",
    "    y_test = [test_feat[1] for test_feat in test_feats]\n",
    "    \n",
    "    # Load classifiers\n",
    "    \n",
    "    # all words\n",
    "    skc_logistic_all = pkl.load(open(folder+'skc_logistic_all.sav', 'rb'))\n",
    "    skc_nb_mult_all = pkl.load(open(folder+'skc_nb_mult_all.sav', 'rb'))\n",
    "    skc_nb_bernoulli_all = pkl.load(open(folder+'skc_nb_bernoulli_all.sav', 'rb'))\n",
    "    #skc_nb_gaussian_all = pkl.load(open(folder+'skc_nb_gaussian_all.sav', 'rb'))\n",
    "    skc_svc_linear_all = pkl.load(open(folder+'skc_svc_linear_all.sav', 'rb'))\n",
    "    #skc_svc_nu_all = pkl.load(open(folder+'skc_svc_nu_all.sav', 'rb'))\n",
    "\n",
    "    # info words\n",
    "    skc_logistic_inf = pkl.load(open(folder+'skc_logistic_inf.sav', 'rb'))\n",
    "    skc_nb_mult_inf = pkl.load(open(folder+'skc_nb_mult_inf.sav', 'rb'))\n",
    "    skc_nb_bernoulli_inf = pkl.load(open(folder+'skc_nb_bernoulli_inf.sav', 'rb'))\n",
    "    #skc_nb_gaussian_inf = pkl.load(open(folder+'skc_nb_gaussian_inf.sav', 'rb'))\n",
    "    skc_svc_linear_inf = pkl.load(open(folder+'skc_svc_linear_inf.sav', 'rb'))\n",
    "    #skc_svc_nu_inf = pkl.load(open(folder+'skc_svc_nu_inf.sav', 'rb'))\n",
    "\n",
    "    # voting\n",
    "    mv_classifier = pkl.load(open(folder+'mv_classifier.sav', 'rb'))\n",
    "    \n",
    "    est_classifiers = [['Logistic - All words',skc_logistic_all],\n",
    "                       ['NB-Multinomial - All words',skc_nb_mult_all],\n",
    "                       ['NB-Bernoulli - All words',skc_nb_bernoulli_all],\n",
    "                       ['SVC-Linear - All words',skc_svc_linear_all],\n",
    "                       ['Logistic - Info words',skc_logistic_inf],\n",
    "                       ['NB-Multinomial - Info words',skc_nb_mult_inf],\n",
    "                       ['NB-Bernoulli - Info words',skc_nb_bernoulli_inf],\n",
    "                       ['SVC-Linear - Info words',skc_svc_linear_inf],\n",
    "                       ['Max-Vote',mv_classifier]]\n",
    "\n",
    "    with open(folder+'performance_NL_'+time.strftime('%Y-%m-%d_%H-%M',time.gmtime())+'.csv', 'w', encoding=\"utf-8\") as csvFile:\n",
    "\n",
    "        csvWriter = csv.writer(csvFile)\n",
    "        csvWriter.writerow('Positive (1): '+str(sum(df.sentiment_pos==1)))\n",
    "        csvWriter.writerow('Negative (0): '+str(sum(df.sentiment_pos==0)))\n",
    "\n",
    "        for est_classifier in est_classifiers:\n",
    "            \n",
    "            print(est_classifier[0])\n",
    "\n",
    "            #calculate            \n",
    "            predicted_classes=est_classifier[1].classify_many(X_test)\n",
    "            \n",
    "            # confusion matrix\n",
    "            cm = confusion_matrix(y_test,predicted_classes)\n",
    "            TN, FP, FN, TP = cm.flatten()\n",
    "            total = TN+FP+FN+TP\n",
    "\n",
    "            # class 1\n",
    "            prec1 = TP / (TP+FP)\n",
    "            reca1 = TP / (TP+FN)\n",
    "            fone1 = 2*(prec1*reca1)/(prec1+reca1)\n",
    "            # class 0\n",
    "            prec0 = TN / (TN+FN)\n",
    "            reca0 = TN / (TN+FP)\n",
    "            fone0 = 2*(prec0*reca0)/(prec0+reca0)\n",
    "\n",
    "            # global / weighted\n",
    "            accuw = TP/total +TN/total\n",
    "            precw = prec0*(TN+FP)/(total) + prec1*(TP+FN)/(total)\n",
    "            recaw = reca0*(TN+FP)/(total) + reca1*(TP+FN)/(total)\n",
    "            fonew = fone0*(TN+FP)/(total) + fone1*(TP+FN)/(total)\n",
    "\n",
    "            # list\n",
    "            sup = [est_classifier[0], TN, FP, FN, TP, prec1, reca1, fone1, prec0, reca0, fone0, precw, recaw, fonew, accuw]\n",
    "            \n",
    "            # write in csv\n",
    "            csvWriter.writerow(sup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
