{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1. Building the Classifiers (_EN = Base File)\n",
    " \n",
    "Daniel Ruiz, MSc in Data Science and Business Analytics (DSBA), Bocconi University\n",
    " \n",
    "Reference codes (alphabetically):\n",
    "- Perkins, Jacob. Python 3 Text Processing with NLTK 3 Cookbook. Packt, 2014.\n",
    "- https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists\n",
    "- https://streamhacker.com/2010/06/16/text-classification-sentiment-analysis-eliminate-low-information-features/\n",
    " \n",
    "## Loading packages and hard data\n",
    " \n",
    "The major steps of Text Classification with Scikit-Learn are:\n",
    "     1. Creating training features (covered in the previous recipes).\n",
    "     2. Choosing and importing an sklearn algorithm.\n",
    "     3. Construct an SklearnClassifier class with the chosen algorithm.\n",
    "     4. Train the SklearnClassifier class with your training features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import time\n",
    "\n",
    "# classification\n",
    "from BOW import *\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.svm import LinearSVC #, NuSVC #, SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Dataset_Twitter_Clean_03/us_EnglishTweets.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ce94db44911b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# open and read file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# overall description of the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(path, compression)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \"\"\"\n\u001b[0;32m    144\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_stringify_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;31m# 1) try standard libary Pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Dataset_Twitter_Clean_03/us_EnglishTweets.pkl'"
     ]
    }
   ],
   "source": [
    "# choosing the language\n",
    "folder='Classifiers/English/'\n",
    "filename='Dataset_Twitter_Clean_03/us_EnglishTweets.pkl'\n",
    "\n",
    "# open and read file\n",
    "df = pd.read_pickle(filename)\n",
    "\n",
    "# overall description of the dataset\n",
    "print('Shape:',df.shape)\n",
    "print('Columns:',df.columns)\n",
    "\n",
    "# balanced ?\n",
    "print('balanced?')\n",
    "print('Sentiments:',df.sentiment_pos.unique())\n",
    "print('Positive (1):',sum(df.sentiment_pos==1))\n",
    "print('Negative (0):',sum(df.sentiment_pos==0))\n",
    "\n",
    "print()\n",
    "if sum(df.sentiment_pos==1) != sum(df.sentiment_pos==0):\n",
    "    sup = min(sum(df.sentiment_pos==1),sum(df.sentiment_pos==0))\n",
    "    df_pos = df[df.sentiment_pos==1].sample(sup)\n",
    "    df_neg = df[df.sentiment_pos==0].sample(sup)\n",
    "    df = pd.concat([df_pos,df_neg]).reset_index(drop=True)\n",
    "    print('balanced')    \n",
    "    print('Positive (1):',sum(df.sentiment_pos==1))\n",
    "    print('Negative (0):',sum(df.sentiment_pos==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building bags-of-words (all words vs most informative words)\n",
    "\n",
    "- lfeats is a dict of lists of dicts\n",
    "- lfeats = collection.defaultdict\n",
    "- lfeats['neg'] = list\n",
    "- lfeats['neg'][0] = dict: String: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "features = df.snowball_stems\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# dataset = all words\n",
    "\n",
    "print('---------------------')\n",
    "print('All words:')\n",
    "\n",
    "# bag_of_words\n",
    "lfeats_all = label_feats_from_corpus(features,\n",
    "                                     df.sentiment_pos,\n",
    "                                     bag_of_words)\n",
    "\n",
    "\n",
    "train_feats_all, test_feats_all= split_label_feats(lfeats_all, split=0.995)\n",
    "\n",
    "print('Training set size:',len(train_feats_all))\n",
    "print('Test set size:',len(test_feats_all))\n",
    "print('---------------------')\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# dataset = informative words only\n",
    "\n",
    "cutoff = 1.5\n",
    "\n",
    "print('---------------------')\n",
    "print('Informative words only:')\n",
    "\n",
    "labeled_words = separate_words(features,\n",
    "                               df.sentiment_pos)\n",
    "\n",
    "high_info_words = set(high_information_words(labeled_words,min_score=cutoff))\n",
    "\n",
    "def bag_of_words_in_set(words, goodwords=high_info_words):\n",
    "    return bag_of_words(set(words) & set(goodwords))\n",
    "\n",
    "lfeats_inf = label_feats_from_corpus(features,\n",
    "                                     df.sentiment_pos,\n",
    "                                     feature_detector=bag_of_words_in_set)\n",
    "\n",
    "train_feats_inf, test_feats_inf = split_label_feats(lfeats_inf, split=0.995)\n",
    "\n",
    "print('Training set size:',len(train_feats_inf))\n",
    "print('Test set size:',len(test_feats_inf))\n",
    "print('---------------------')\n",
    "print('Words (all):',len(set(labeled_words[0][1]+labeled_words[1][1])))\n",
    "print('Words (high info):',len(high_info_words))\n",
    "print('---------------------')\n",
    "\n",
    "\n",
    "# ### optimal cutoff = 1.5\n",
    "# \n",
    "# print('Optimizing the number of informative words')\n",
    "# \n",
    "# labeled_words = separate_words(features,\n",
    "#                                df.sentiment_pos)\n",
    "# \n",
    "# acc = []\n",
    "# \n",
    "# \n",
    "# for i in range(12,31):\n",
    "# \n",
    "#     high_info_words = set(high_information_words(labeled_words,min_score=i/10))\n",
    "# \n",
    "#     def bag_of_words_in_set(words, goodwords=high_info_words):\n",
    "#         return bag_of_words(set(words) & set(goodwords))\n",
    "# \n",
    "#     lfeats_inf = label_feats_from_corpus(features,\n",
    "#                                          df.sentiment_pos,\n",
    "#                                          feature_detector=bag_of_words_in_set)\n",
    "# \n",
    "#     train_feats_inf, test_feats_inf = split_label_feats(lfeats_inf, split=0.99)\n",
    "#         \n",
    "#     skc_logistic_inf = SklearnClassifier(LogisticRegression(solver='lbfgs',max_iter=2000))\n",
    "#     skc_logistic_inf.train(train_feats_inf)\n",
    "#     \n",
    "#     acc.append(accuracy(skc_logistic_inf, test_feats_inf))\n",
    "#     \n",
    "#     print(i/10,accuracy(skc_logistic_inf, test_feats_inf),len(high_info_words))\n",
    "# \n",
    "# cutoff = [i for i in range(12,31)][acc.index(max(acc))]/10\n",
    "# \n",
    "# print('optimal cutoff:',cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running classifiers\n",
    "- We do not compute the traditional SVC or the Decision Tree classifiers because they performed substantially worse and took longer to reach equilibrium in the book examples. Instead of the traditional SVC, we compute the linear SVC and the NuSVC, which obtained superior performance in the 'cookbook' text classification tasks.\n",
    " \n",
    "### With all words (i.e. informative and non-informative)\n",
    "- The SklearnClassifier converts NLTK feature dictionaries into into sklearn compatible feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "skc_logistic_all = SklearnClassifier(LogisticRegression(solver='lbfgs',max_iter=2000))\n",
    "skc_logistic_all.train(train_feats_all)\n",
    "print('Logistic: done!')\n",
    "\n",
    "# Logistic Regression CV\n",
    "#skc_logistic_cv_all = SklearnClassifier(LogisticRegressionCV(solver='lbfgs',max_iter=2000))\n",
    "#skc_logistic_cv_all.train(train_feats_all)\n",
    "#print('Logistic-CV: done!')\n",
    "\n",
    "# Naive Bayes Classifier for multivariate Multinomial models (i.e. occurrence counts)\n",
    "skc_nb_mult_all = SklearnClassifier(MultinomialNB())\n",
    "skc_nb_mult_all.train(train_feats_all)\n",
    "print('NB-Mult: done!')\n",
    "\n",
    "# Naive Bayes Classifier for multivariate Bernoulli models (i.e. binary response)\n",
    "skc_nb_bernoulli_all= SklearnClassifier(BernoulliNB())\n",
    "skc_nb_bernoulli_all.train(train_feats_all)\n",
    "print('NB-Bernoulli: done!')\n",
    "\n",
    "# Naive Bayes Classifier for Gaussian models\n",
    "#skc_nb_gaussian_all= SklearnClassifier(GaussianNB())\n",
    "#skc_nb_gaussian_all.train(train_feats_all)\n",
    "#print('NB-Gaussian: done!')\n",
    "\n",
    "# LinearSVC: Special Implementation of Support Vector Classifier with Linear Kernel\n",
    "skc_svc_linear_all = SklearnClassifier(LinearSVC(max_iter=2000))\n",
    "skc_svc_linear_all.train(train_feats_all)\n",
    "print('Linear-SVC: done!')\n",
    "\n",
    "# NuSVC: Support Vector Classifier with Restricted Number of Support Vectors\n",
    "#skc_svc_nu_all = SklearnClassifier(NuSVC())\n",
    "#skc_svc_nu_all.train(train_feats_all)\n",
    "#print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With informative words only\n",
    "- If there are some words that are biased towards the 'pos' label, but still occur every now and then at the 'neg' label, this could cause some confusion in your algorithm. To correct this behavior, we use only the most informative words.\n",
    "- \"Its accuracy before was 86.4%, so we actually got a very slight decrease. In general, support vector machine and logistic regression-based algorithms will benefit less, or perhaps even be harmed, by pre-filtering the training features. This is because these algorithms are able to learn feature weights that correspond to the significance of each feature, whereas Naive Bayes algorithms do not.\"\n",
    "- \"A high information word is a word that is strongly biased towards a single classification label. The low information words are words that are common to all labels. It may be counter-intuitive, but eliminating these words from the training data can actually improve accuracy, precision, and recall. The reason this works is that using only high information words reduces the noise and confusion of a classifier's internal model. If all the words/features are highly biased one way or the other, it's much easier for the classifier to make a correct guess.\"\n",
    "- The labeled test features = gold standard\n",
    "- \"The default score_fn is nltk.metrics.BigramAssocMeasures.chi_sq(), ff n_ii: This is the frequency of the word for the label ff n_ix: This is the total frequency of the word across all labels ff n_xi: This is the total frequency of all words that occurred for the label ff n_xx: This is the total frequency for all words in all labels\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "skc_logistic_inf = SklearnClassifier(LogisticRegression(solver='lbfgs',max_iter=2000))\n",
    "skc_logistic_inf.train(train_feats_inf)\n",
    "print('Logistic: done!')\n",
    "\n",
    "# Logistic Regression CV\n",
    "#skc_logistic_cv_inf = SklearnClassifier(LogisticRegressionCV(solver='lbfgs',max_iter=2000))\n",
    "#skc_logistic_cv_inf.train(train_feats_inf)\n",
    "#print('Logistic-CV: done!')\n",
    "\n",
    "# Naive Bayes Classifier for multivariate Multinomial models (i.e. occurrence counts)\n",
    "skc_nb_mult_inf = SklearnClassifier(MultinomialNB())\n",
    "skc_nb_mult_inf.train(train_feats_inf)\n",
    "print('NB-Mult: done!')\n",
    "\n",
    "# Naive Bayes Classifier for multivariate Bernoulli models (i.e. binary response)\n",
    "skc_nb_bernoulli_inf= SklearnClassifier(BernoulliNB())\n",
    "skc_nb_bernoulli_inf.train(train_feats_inf)\n",
    "print('NB-Bernoulli: done!')\n",
    "\n",
    "# Naive Bayes Classifier for Gaussian models\n",
    "#skc_nb_gaussian_inf= SklearnClassifier(GaussianNB())\n",
    "#skc_nb_gaussian_inf.train(train_feats_inf)\n",
    "#print('NB-Gaussian: done!')\n",
    "\n",
    "# LinearSVC: Special Implementation of Support Vector Classifier with Linear Kernel\n",
    "skc_svc_linear_inf = SklearnClassifier(LinearSVC(max_iter=2000))\n",
    "skc_svc_linear_inf.train(train_feats_inf)\n",
    "print('Linear-SVC: done!')\n",
    "\n",
    "# NuSVC: Support Vector Classifier with Restricted Number of Support Vectors\n",
    "#skc_svc_nu_inf = SklearnClassifier(NuSVC(max_iter=2000))\n",
    "#skc_svc_nu_inf.train(train_feats_inf)\n",
    "#print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Classifiers with Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_classifier = MaxVoteClassifier(skc_nb_mult_all,\n",
    "                                  skc_nb_bernoulli_all,\n",
    "                                  skc_logistic_all,\n",
    "                                  skc_svc_linear_all,\n",
    "                                  skc_nb_mult_inf,\n",
    "                                  skc_nb_bernoulli_inf,\n",
    "                                  skc_logistic_inf,\n",
    "                                  skc_svc_linear_inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# classifiers\n",
    "est_classifiers = [['Logistic - All words',skc_logistic_all,test_feats_all],\n",
    "                   ['NB-Multinomial - All words',skc_nb_mult_all,test_feats_all],\n",
    "                   ['NB-Bernoulli - All words',skc_nb_bernoulli_all,test_feats_all],\n",
    "                   ['SVC-Linear - All words',skc_svc_linear_all,test_feats_all],\n",
    "                   ['Logistic - Info words',skc_logistic_inf,test_feats_inf],\n",
    "                   ['NB-Multinomial - Info words',skc_nb_mult_inf,test_feats_inf],\n",
    "                   ['NB-Bernoulli - Info words',skc_nb_bernoulli_inf,test_feats_inf],\n",
    "                   ['SVC-Linear - Info words',skc_svc_linear_inf,test_feats_inf],\n",
    "                   ['Max-Vote',mv_classifier,test_feats_all]]\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# save\n",
    "pkl.dump(skc_logistic_all, open(folder+'skc_logistic_all.sav', 'wb'))\n",
    "#pkl.dump(skc_logistic_cv_all, open(folder+'skc_logistic_cv_all.sav', 'wb'))\n",
    "pkl.dump(skc_nb_mult_all, open(folder+'skc_nb_mult_all.sav', 'wb'))\n",
    "pkl.dump(skc_nb_bernoulli_all, open(folder+'skc_nb_bernoulli_all.sav', 'wb'))\n",
    "#pkl.dump(skc_nb_gaussian_all, open(folder+'skc_nb_gaussian_all.sav', 'wb'))\n",
    "pkl.dump(skc_svc_linear_all, open(folder+'skc_svc_linear_all.sav', 'wb'))\n",
    "#pkl.dump(skc_svc_nu_all, open(folder+'skc_svc_nu_all.sav', 'wb'))\n",
    "# save\n",
    "pkl.dump(skc_logistic_inf, open(folder+'skc_logistic_inf.sav', 'wb'))\n",
    "#pkl.dump(skc_logistic_CV_inf, open(folder+'skc_logistic_cv_inf.sav', 'wb'))\n",
    "pkl.dump(skc_nb_mult_inf, open(folder+'skc_nb_mult_inf.sav', 'wb'))\n",
    "pkl.dump(skc_nb_bernoulli_inf, open(folder+'skc_nb_bernoulli_inf.sav', 'wb'))\n",
    "#pkl.dump(skc_nb_gaussian_inf, open(folder+'skc_nb_gaussian_inf.sav', 'wb'))\n",
    "pkl.dump(skc_svc_linear_inf, open(folder+'skc_svc_linear_inf.sav', 'wb'))\n",
    "#pkl.dump(skc_svc_nu_inf, open(folder+'skc_svc_nu_inf.sav', 'wb'))\n",
    "# save\n",
    "pkl.dump(mv_classifier, open(folder+'mv_classifier.sav', 'wb'))\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# write down performance\n",
    "\n",
    "with open(folder+'performance_in_similar_'+time.strftime('%Y-%m-%d_%H-%M',time.gmtime())+'.csv', 'w', encoding=\"utf-8\") as csvFile:\n",
    "    \n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    \n",
    "    for est_classifier in est_classifiers:\n",
    "        \n",
    "        X_test = [f[0] for f in est_classifier[2]]\n",
    "        y_test = [f[1] for f in est_classifier[2]]\n",
    "        \n",
    "        predicted_classes=est_classifier[1].classify_many(X_test)\n",
    "\n",
    "        # confusion matrix\n",
    "        cm = confusion_matrix(y_test,predicted_classes)\n",
    "        TN, FP, FN, TP = cm.flatten()\n",
    "        total = TN+FP+FN+TP\n",
    "\n",
    "        # class 1\n",
    "        prec1 = TP / (TP+FP)\n",
    "        reca1 = TP / (TP+FN)\n",
    "        fone1 = 2*(prec1*reca1)/(prec1+reca1)\n",
    "        # class 0\n",
    "        prec0 = TN / (TN+FN)\n",
    "        reca0 = TN / (TN+FP)\n",
    "        fone0 = 2*(prec0*reca0)/(prec0+reca0)\n",
    "\n",
    "        # global / weighted\n",
    "        accuw = TP/total +TN/total\n",
    "        precw = prec0*(TN+FP)/(total) + prec1*(TP+FN)/(total)\n",
    "        recaw = reca0*(TN+FP)/(total) + reca1*(TP+FN)/(total)\n",
    "        fonew = fone0*(TN+FP)/(total) + fone1*(TP+FN)/(total)\n",
    "\n",
    "        # list\n",
    "        sup = [est_classifier[0], TN, FP, FN, TP, prec1, reca1, fone1, prec0, reca0, fone0, precw, recaw, fonew, accuw]\n",
    "\n",
    "        # write in csv\n",
    "        csvWriter.writerow(sup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Model Visualization\n",
    "### Classifying a few tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# select one classifier\n",
    "classifier = skc_logistic_inf\n",
    "# two examples\n",
    "print('valoriza ->', classifier.classify(bag_of_words(['valoriza'])))\n",
    "probs = classifier.prob_classify(bag_of_words(['valoriza']))\n",
    "print('Categories:',probs.samples())\n",
    "print('Classification given:', probs.max())\n",
    "print('Probability of being positive:',probs.prob(1))\n",
    "print('Probability of being negative:',probs.prob(0))\n",
    "print('Ratio:', probs.prob(1)/probs.prob(0))\n",
    "print('\\n-------------------------\\n')\n",
    "print('chateada ->', classifier.classify(bag_of_words(['chateada'])))\n",
    "probs = classifier.prob_classify(bag_of_words(['chateada']))\n",
    "print('Categories:',probs.samples())\n",
    "print('Classification given:', probs.max())\n",
    "print('Probability of being positive:',probs.prob(1))\n",
    "print('Probability of being negative:',probs.prob(0))\n",
    "print('Ratio:', probs.prob(1)/probs.prob(0))\n",
    "print('\\n-------------------------\\n')\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# most informative words\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_feats_all)\n",
    "print(\"Classifier accuracy percent:\",(accuracy(classifier, test_feats_all))*100)\n",
    "\n",
    "# Show most informative features with ratios (ratio = p('pos')/p('neg') for a review of only one word)\n",
    "print(classifier.show_most_informative_features(n=50))\n",
    "print('\\n-------------------------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
